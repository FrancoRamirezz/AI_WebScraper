# -*- coding: utf-8 -*-
"""AI webcrawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RYNgLwtDlOiAcflAo7Ju4tNukWJNZTOR
"""

# install the openai stuff
! pip install openai

! pip install tiktoken
! pip install cohere

!pip install -qU \
    openai==0.28.0 \
    langchain==0.0.301 \
    fastapi==0.103.1 \
    "uvicorn[standard]"==0.23.2

! pip install langchain

!pip install langchain --upgrade

!pip install tiktoken
! pip install playwright

import os
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.schema import HumanMessage
from langchain.document_loaders import WebBaseLoader

import os
os.environ["OPENAI_API_KEY"] = "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"
2

from langchain.memory import ConversationBufferWindowMemory
from langchain.agents import load_tools, AgentType, initialize_agent
from langchain.document_loaders import AsyncChromiumLoader
from langchain.document_transformers import BeautifulSoupTransformer
from langchain.document_loaders import AsyncHtmlLoader
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")

"""One version of scraping the entire website

**This is a simple version if we are looking for a specific aspect of the website**
"""

from langchain.document_loaders import AsyncHtmlLoader
urls = ["https://www.ebay.com/"]
loader = AsyncHtmlLoader(urls) # this is used to make request
docs = loader.load()
print(docs)

"""Extracting with Schema

This is used for specific extraction needed
"""

from langchain.chains import create_extraction_chain

schema = {
      "properties": {
        "product_name": {"type": "string"},
        "item_price": {"type": "intger"},
    },
    "required": ["product_name", "item_price"],
}

def extract(content: str, schema: dict):
    return create_extraction_chain(schema=schema, llm=llm).run(content)

"""**Now putting both cells togther**

## make sure not to run this part a lot
"""

from langchain import schema
from langchain.document_transformers import BeautifulSoupTransformer
async def parsing_info(url:str, schema) -> str and str:
  loader = AsyncHtmlLoader(url)
  docs = loader.load()
  bot_Soup= BeautifulSoupTransformer()
  docs_transformed = schema.transform_documents(docs,tags_to_extract=["span"])
  schema = {
      "properties": {
        "product_name": {"type": "string"},
        "item_price": {"type": "intger"},
    },
    "required": ["product_name", "item_price"],
  }
  # we will set a function within another function to ensure both the extraction is being used correctly
  def ebay_content(content:str, schema:dict ) -> str and dict:
    return create_extraction_chain(sc)
response = await parsing_info("https://www.ebay.com/", schema = schema)
print(response)

"""A finer tuned verion here to look on a diffrent website"""

# common error give is the event loop that run in the background, use the async in front of the
import pprint
from langchain.text_splitter import RecursiveCharacterTextSplitter
# run this part in other part text editor
class scrape():
  def __init__():
    async def scrape_starup(urls, schema):
      loader = AsyncChromiumLoader(urls)
      docs = loader.load()
      bs_transformer = BeautifulSoupTransformer()
      docs_transformed = bs_transformer.transform_documents(
        docs, tags_to_extract=["span"] # this is used to extract the exact html tags needed
    )
    print("Scraping the context ")

    # for the tokens needed
    tokens = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000, chunk_overlap=0
    )
    splits = token.split_documents(docs_transformed)

    # Process the first split
    extracted_content = extract(schema=schema, content=splits[0].page_content)
    pprint.pprint(extracted_content)
    return extracted_content


urls = ["https://news.ycombinator.com/newstartups"]
extracted_content = scrape_with_playwright(urls, schema=schema)